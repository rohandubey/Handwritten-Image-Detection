# Handwritten-Image-Detection
## Abstract
Handwriting recognition is a crucial task in the field of computer vision, but it can be a challenging problem to solve. It involves accurately identifying and translating handwritten characters in an image into digital text. In recent years, generative adversarial networks (GANs) have been utilised extensively for this purpose, as they can generate high-quality images that closely resemble the training data. However, GANs have some limitations, such as the need for a significant amount of training data and the difficulty in regulating the generated images. This paper presents a novel approach for handwriting recognition that combines deep convolution GAN (DCGAN) model with optical feature extraction techniques, including Scale Invariant Feature Transform (SIFT) and Oriented FAST and Rotated BRIEF (ORB). Our method uses both the image and SIFT or ORB optical features as inputs to a network that has been trained to recognize the shape and orientation of characters from each class. This study examines the effect of both regular and dense optical features. We evaluate our algorithm on the MNIST, EMNIST, and MADBase datasets. The results indicate that GANs using optical feature approaches are superior to conventional GANs, with an accuracy improvement of 4.88% in the MNSIT dataset, 6.89% in the EMNIST dataset and 0.62% in the MADBase dataset. In addition, our approach achieves state-of-the-art performance on the MADBase dataset.
> Codes will be released once the paper gets published!
